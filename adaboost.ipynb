{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIKTpH3dReq1mrNbYaSjpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jyothika083/ML/blob/main/adaboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sT6NKgbB8McD"
      },
      "outputs": [],
      "source": [
        "# Compute error rate, alpha and w\n",
        "def compute_error(y, y_pred, w_i):\n",
        "    '''\n",
        "    Calculate the error rate of a weak classifier m. Arguments:\n",
        "    y: actual target value\n",
        "    y_pred: predicted value by weak classifier\n",
        "    w_i: individual weights for each observation\n",
        "\n",
        "    Note that all arrays should be the same length\n",
        "    '''\n",
        "    return (sum(w_i * (np.not_equal(y, y_pred)).astype(int)))/sum(w_i)\n",
        "\n",
        "def compute_alpha(error):\n",
        "    '''\n",
        "    Calculate the weight of a weak classifier m in the majority vote of the final classifier. This is called\n",
        "    alpha in chapter 10.1 of The Elements of Statistical Learning. Arguments:\n",
        "    error: error rate from weak classifier m\n",
        "    '''\n",
        "    return np.log((1 - error) / error)\n",
        "\n",
        "def update_weights(w_i, alpha, y, y_pred):\n",
        "    '''\n",
        "    Update individual weights w_i after a boosting iteration. Arguments:\n",
        "    w_i: individual weights for each observation\n",
        "    y: actual target value\n",
        "    y_pred: predicted value by weak classifier\n",
        "    alpha: weight of weak classifier used to estimate y_pred\n",
        "    '''\n",
        "    return w_i * np.exp(alpha * (np.not_equal(y, y_pred)).astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define AdaBoost class\n",
        "class AdaBoost:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.alphas = []\n",
        "        self.G_M = []\n",
        "        self.M = None\n",
        "        self.training_errors = []\n",
        "        self.prediction_errors = []\n",
        "\n",
        "    def fit(self, X, y, M = 100):\n",
        "        '''\n",
        "        Fit model. Arguments:\n",
        "        X: independent variables - array-like matrix\n",
        "        y: target variable - array-like vector\n",
        "        M: number of boosting rounds. Default is 100 - integer\n",
        "        '''\n",
        "\n",
        "        # Clear before calling\n",
        "        self.alphas = []\n",
        "        self.training_errors = []\n",
        "        self.M = M\n",
        "\n",
        "        # Iterate over M weak classifiers\n",
        "        for m in range(0, M):\n",
        "\n",
        "            # Set weights for current boosting iteration\n",
        "            if m == 0:\n",
        "                w_i = np.ones(len(y)) * 1 / len(y)  # At m = 0, weights are all the same and equal to 1 / N\n",
        "            else:\n",
        "                # (d) Update w_i\n",
        "                w_i = update_weights(w_i, alpha_m, y, y_pred)\n",
        "\n",
        "            # (a) Fit weak classifier and predict labels\n",
        "            G_m = DecisionTreeClassifier(max_depth = 1)     # Stump: Two terminal-node classification tree\n",
        "            G_m.fit(X, y, sample_weight = w_i)\n",
        "            y_pred = G_m.predict(X)\n",
        "\n",
        "            self.G_M.append(G_m) # Save to list of weak classifiers\n",
        "\n",
        "            # (b) Compute error\n",
        "            error_m = compute_error(y, y_pred, w_i)\n",
        "            self.training_errors.append(error_m)\n",
        "\n",
        "            # (c) Compute alpha\n",
        "            alpha_m = compute_alpha(error_m)\n",
        "            self.alphas.append(alpha_m)\n",
        "\n",
        "        assert len(self.G_M) == len(self.alphas)\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Predict using fitted model. Arguments:\n",
        "        X: independent variables - array-like\n",
        "        '''\n",
        "\n",
        "        # Initialise dataframe with weak predictions for each observation\n",
        "        weak_preds = pd.DataFrame(index = range(len(X)), columns = range(self.M))\n",
        "\n",
        "        # Predict class label for each weak classifier, weighted by alpha_m\n",
        "        for m in range(self.M):\n",
        "            y_pred_m = self.G_M[m].predict(X) * self.alphas[m]\n",
        "            weak_preds.iloc[:,m] = y_pred_m\n",
        "\n",
        "        # Calculate final predictions\n",
        "        y_pred = (1 * np.sign(weak_preds.T.sum())).astype(int)\n",
        "\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "mZe8FjFu8TQc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "hQPRxc-Z8T0U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Dataset\n",
        "df = pd.read_csv('/content/spambase.data', header = None)\n",
        "\n",
        "# Column names\n",
        "names = pd.read_csv('/content/spambase.names', sep = ':', skiprows=range(0, 33), header = None)\n",
        "col_names = list(names[0])\n",
        "col_names.append('Spam')\n",
        "\n",
        "# Rename df columns\n",
        "df.columns = col_names\n",
        "\n",
        "# Convert classes in target variable to {-1, 1}\n",
        "df['Spam'] = df['Spam'] * 2 - 1\n",
        "\n",
        "# Train - test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = 'Spam').values,\n",
        "                                                    df['Spam'].values,\n",
        "                                                    train_size = 3065,\n",
        "                                                    random_state = 2)"
      ],
      "metadata": {
        "id": "MXuxuDDQ8XlN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fit model\n",
        "ab = AdaBoost()\n",
        "ab.fit(X_train, y_train, M = 400)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = ab.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je4Bz2xH8aBN",
        "outputId": "efcca9d4-2a92-44c3-bef4-8efde5a0c234"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9440104166666666\n"
          ]
        }
      ]
    }
  ]
}